---
title: "CRD"
format: html
---

# Introduction  
The goals of this exercise are to:  
- Create an analytical workflow for a CRD design, from data import through publication-ready plot  
- Understand each of its components  
- Talk about some important aspects including   
    - contrast type, 
    - sum-of-squares type
    - deciding how to extract and display model means and pairwise comparisons  

# a) Setup  
Here is where we load the packages we will use.  
```{r setup}
#install.packages("broom")

install.packages("tidyverse")
install.packages("car")
install.packages("broom")
install.packages("emmeans")
install.packages("multcomp")

# Loading packages

library(tidyverse) # for data wrangling and plotting
library(car) # for Anova function
library(broom) # for model residuals extraction and residuals diagnostics
library(emmeans) # for model mean extraction
library(multcomp) # for pairwise comparison letter display
```

```{r data import}
crd_df <- read_csv("../data/wheat_nk_balkh.csv")

crd_df
```

# b) EDA tables  
```{r summary}
summary(crd_df)
```

```{r glimpse}
glimpse(crd_df)
```

# c) Wrangling  
```{r crd_dfw}
crd_dfw <- crd_df %>% #The "w" in "crd_dfw" is to indicate wrangled dataframe/ object
  mutate(rep = factor(rep), #"factor()" function converts "rep" from a "numeric" variable to "categorical" variable
         nrate_kgha = factor(nrate_kgha), #"factor()" function converts "numeric" variable to "categorical" variable
         krate_kgha = factor(krate_kgha) #"factor()" function converts "numeric" variable to "categorical" variable
         ) %>%
  mutate(trtname = paste0(nrate_kgha,"+",krate_kgha)) #to combine the "nrate_kgha" and "krate_kgha" treatments, then treat the combined variable "trtname" as a factor


crd_dfw
```


```{r crd_dfw}
summary(crd_dfw)
```
Number of replicates: 4  
Number o treatments: 3 N rates x 3 K rates = 9  
Number of observations: 4 x 9 = 36  
Yield: from 2795 to 7445 kg/ha  

# d) EDA (Exploratory Data Analysis) plots  
```{r n boxplot}
ggplot(crd_dfw, aes(x = nrate_kgha,
                    y = yield_kgha,
                    color = nrate_kgha #To color the boxplots by "nrate_kgha"
                    )) +
  geom_boxplot() + #"geom_boxplot()" function creates boxplots
  geom_point() + # "geom_point()" function shows all data points in a straight line in the boxplots
  geom_jitter() + # "geom_jitter" function jitters out all the data points in a horizontal spread rather than a     straight line. This is helpful because a lot of times (e.g., machine learning) all data points in a straight line does not help.
  theme(legend.position = "none") # "theme(legend.position = "none")" function removes the legend from the graph. In this example, we do not need legends because the levels of "nrate_kgha" is already indicated in the x-axis
```



```{r k boxplot}
ggplot(crd_dfw, aes(x = krate_kgha, 
                    y = yield_kgha,
                    color = krate_kgha)) +
  geom_boxplot() +
  geom_jitter() +
  theme(legend.position = "none")
```

```{r nk boxplot}
ggplot(crd_dfw, aes(x = nrate_kgha,
                    y = yield_kgha,
                    color = nrate_kgha
                    )) +
  geom_boxplot() +
  geom_jitter() +
  facet_grid(.~krate_kgha) + #takes all levels of a specified variable, then creates separate boxplots for each of the variable levels 
  theme(legend.position = "none") #removes the legends from the graph

```

# e) Statistical model  
## Set-to-zero vs. sum-to-zero  
In R, the default contrast type is *set-to-zero*.  

In research, we normally are interested in *sum-to-zero* contrasts.  

Below we change the default to sum-to-zero ("contr.sum") before fitting the model.


```{r model}
# Changing to sum-to-zero contrast
options(contrasts = c("contr.sum", "contr.poly"))

# Model fitting
crd_mod <- lm(yield_kgha ~ nrate_kgha + krate_kgha +
                nrate_kgha:krate_kgha,
              data = crd_dfw
                )

crd_dfw
# Summary
summary(crd_mod)
```

## Model specification tips  
Instead of specifying 
          `nrate_kgha + krate_kgha + nrate_kgha:krate_kgha`,  
we could have just used  
                   `nrate_kgha*krate_kgha`.  

R interprets the `*` as "give me all main effects and interactions".

The more interacting effects a model has, the more efficient using the * becomes.  


# f) ANOVA table  
The `Anova()` function (case sensitive) allows us to use **type 3 sum of squares**.  

The common functions `aov()` and `anova()` use type 1 SS, which is the wrong choice IF have unbalanced data.

If data is balanced, type 1 and 3 give same results.  

For sake of peace of mind, it is simpler to just always use type 3.  

```{r ANOVA}
Anova(crd_mod, type = 3)
```
In the ANOVA output, *(Intercept)* indicates the *overall mean*. We do NOT indicate this in the ANOVA table.

Whenever your model has interaction terms, need to look at them first.  

IF interaction term is significant, then you should explore that effect, and not the main effects separately in case they are also significant.    

In our case, we have a **significant effect of the interaction between nrate_kgha and krate_kgha**, and the main effects are not significant.

Therefore, we should extract means and perform pairwise comparisons for the interaction term.

Before we do that, let's check our model assumptions. Remember, a model is only valid for inference (i.e., means and pwc) IF if fulfills the linear model assumptions.  


# g) Linear model assumptions  
## Extracting residuals
First, let's extract our model residuals, and also create studentized residuals.  

```{r crd_resid}
crd_resid <- augment(crd_mod) %>% #"augment() function is included in the "broom" package
  mutate(.studresid = rstudent(crd_mod)) #creating studentized residuals in a separate column

crd_resid
```

Notice how we have a data frame with the original data rows and columns, plus some extra residual information.  

Now, let's recap the linear model assumptions:  

- Residual independence (no pattern)  
- Residual variance homogeneity (homoscedasticity) (meaning that the variance around the residuals are basically similar, it looks like a shotgun, like a cloud of points and no specific pattern of increasing/ decreasing variance )
- Residual normality (discuss this!)  
- Outlier detection (< -3 SD or > 3 SD)  

## Residual independence  
- For this, we use the **fitted vs. residual plot**.  
- What we want to see: no clear pattern in residuals, random cloud of points.  
- What we do not want to see: clear pattern, for example, quadratic shape.  
- Adding a `geom_smooth()` helps with that conclusion. We want to see the smooth line and error bands comprising 0 on the y axis.  

```{r }
ggplot(crd_resid, aes(x = .fitted,
                      y = .studresid)) +
  geom_point(shape = 21, #shape of the points
             fill = "purple", #color of the points
             size = 3, #size of the points
             alpha = .7 #"alpha" specifies the transparency of the points; ranges from 0 to 1 - 0 being completely transparent, 1 is a solid color; here we used .7 for 30 % transparency
              ) +
  geom_hline(yintercept = c(-3, 0, 3), color = "red") + #"geom_hline" adds horizontal reference lines #geom_vline => adds line vertically; geom_hline => adds line horizontally
  geom_smooth() #spline regression line to visualize residual pattern
```
Looks great! Next.  

## Residual homoscedasticity  
- For this, we use the **fitted vs. residual plot**.  
- What we want to see: no clear pattern in residuals, random cloud of points.  
- What we do not want to see: residuals increasing as fitted value increases (fan shape).  
- Adding a `geom_smooth()` helps with that conclusion. We want to see the smooth line and error bands comprising 0 on the y axis.  

```{r }
ggplot(crd_resid, aes(x=.fitted, y=.studresid))+
  geom_hline(yintercept = 0, color="red")+
  geom_point(shape = 21,
             fill = "purple", 
             size = 3,
             alpha = .7)+
  geom_smooth()+
  geom_hline(yintercept = c(-3,3), color = "red")+
  theme_bw()
```
Looks great! Next.  

## Residual normality  
- For this, we use the **quantile-quantile (QQ) plot** and **density plot**.    
- What we want to see: residuals centered around 0 and following a normal distribution.  
- What we do not want to see: skewed residuals that do not follow a normal distribution.  

On the QQ plot, we want to see residuals on the black line, meaning they follow their theoretical normal distribution.  
```{r}

```

It's common for some residuals in the tails being off, especially with low N (N=36). Nothing to worry here.  


```{r}


```
Although the density is a bit higher than expected at the tails, it still looks ok.  

Next.  

## Residual outliers  
- For this, we use the **fitted vs. residual plot**.  
- What we want to see: most if not all residuals within [-3,3] on a studentized residual scale.  
- What we do not want to see: too many residuals > 3 or < -3, the farther away form the thresholds the worse.  
- Adding a `geom_hline()` at the thresholds helps to visualize and diagnose.   

```{r}
ggplot(crd_resid, aes(x=.fitted, y=.studresid))+
  geom_hline(yintercept = 0, color="red")+
  geom_point(shape = 21,
             fill = "purple", 
             size = 3,
             alpha = .7)+
  geom_smooth()+
  geom_hline(yintercept = c(-3,3), color = "red")+
  theme_bw()

```
All residuals are within the [-3, 3] interval, so nothing to worry here.  
Now that model assumptions have been checked and met, we can proceed to using the model for inference.  

# h) Model means  
The next step in the workflow is extracting the model means.  

Whenever we are showing means (in tables or plots), we want them to be from a model, and not simply the arithmetic mean in the raw data (like we would get with `group_by()` and `summarise()`).  

This is specially important IF the data is unbalanced (i.e., missing data), in which case model means are DIFFERENT from arithmetic means on raw data. 

Also, when extracting means from an interaction, there are few different ways of doing it, and which one we do depends on the study objectives. Let's explore them below.

```{r interaction means all}
crd_means_all 

crd_means_all
```

```{r interaction means n inside k}
crd_means_nk 

crd_means_nk
```

```{r interaction means k inside n}
crd_means_kn 

crd_means_kn
```

Notice how the 3 different approaches create structure in the mean extraction, which will carry over to the pwc step.  

# i) Pairwise comparisons  
Now that we extracted means, let's perform pairwise comparisons among them.  

First, let's extract means for all 3 types of interaction means extracted above. After that, we'll make a decision on which one to use.

```{r interaction pwc all}
crd_cld_all <- 

crd_cld_all
```

```{r interaction pwc n inside k}
crd_cld_nk <- cld(crd_means_nk, 
                   reversed=T, 
                   adjust="none",
               Letters=letters)

crd_cld_nk
```

```{r interaction pwc k inside n}
crd_cld_kn <- cld(crd_means_kn, 
                   reversed=T, 
                   adjust="none",
               Letters=letters) 

crd_cld_kn
```

Notice how different types of pwc (which are actually coming from differences in how we extracted the means) are testing different hypothesis.  

I would like to test the hypothesis of everything compared to everything else, which corresponds to our first method using `:`. Let's do that below and some light wrangling.    

```{r selected pwc}
crd_cld_selected <- crd_cld_all 

crd_cld_selected
```

## g) Final plot  
Let's plot our results, including both **raw data** (for allowing our audience to inspect data distribution) and **statistical model summary (i.e., letter separation)** for inference purposes.    

```{r crd final plot}

  # Raw data and boxplots  

  # Adding letters

```

Let's make this plot publication ready.  

Also, let's explore some different ways of showing this, and how that may impact interpretation, using facets.  


